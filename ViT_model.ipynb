{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision, torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, hid_size, p_drop=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim, hid_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hid_size, dim),\n",
    "            nn.Dropout(p_drop))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, p_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.hid_size = dim // n_heads\n",
    "        self.n_heads = n_heads\n",
    "        self.scale = np.sqrt(self.hid_size)\n",
    "        self.first_linear = nn.Linear(dim, 3*n_heads*self.hid_size, bias=False)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(n_heads*self.hid_size, dim, bias=False),\n",
    "            nn.Dropout(p=p_drop))\n",
    "    \n",
    "    def make_attention(self, q, k, v):\n",
    "        A = F.softmax(torch.matmul(q, k.permute(0, 2, 1)) / self.scale, dim=-1)\n",
    "        return torch.matmul(A, v)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.first_linear(inputs)\n",
    "        x = torch.cat([self.make_attention(x[:, :, 3*i*self.hid_size:(3*i + 1)*self.hid_size],\n",
    "                    x[:, :, (3*i + 1)*self.hid_size:(3 * i + 2)*self.hid_size],\n",
    "                    x[:, :, (3*i + 2)*self.hid_size:(3 * i + 3)*self.hid_size]) for i in range(self.n_heads)],\n",
    "                                                                                                         dim=2)\n",
    "        return self.out(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, mlp_size, n_patches, n_heads, p_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = Attention(dim, n_heads, p_drop)\n",
    "        self.mlp = MLP(dim, mlp_size, p_drop)\n",
    "        self.ln1 = nn.LayerNorm([n_patches + 1, dim])\n",
    "        self.ln2 = nn.LayerNorm([n_patches + 1, dim])\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.attention(self.ln1(inputs)) + inputs      \n",
    "        return self.mlp(self.ln2(x)) + x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, n_classes, dim, mlp_size, n_patches, patch_size=16, depth=8, n_heads=8,\n",
    "                n_channels=3, p_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.init_linear = nn.Linear(patch_size**2 * n_channels, dim, bias=False)\n",
    "        self.xclass = nn.Parameter(torch.zeros(1,1,dim))\n",
    "        self.E_pos = nn.Parameter(torch.randn(n_patches + 1, dim) * 0.02)\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "        self.transformer = nn.Sequential(*[TransformerEncoderBlock(dim, mlp_size, n_patches,\n",
    "                                                                   n_heads, p_drop) for _ in range(depth)])\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(dim),\n",
    "                                      nn.Linear(dim, n_classes))\n",
    "        \n",
    "    def patching_images(self, inputs):\n",
    "        b, c, h, w = inputs.shape\n",
    "        assert h % self.patch_size == 0 and w % self.patch_size == 0, \"Need to change patch_size\"\n",
    "        x = inputs.reshape(b, c, h // self.patch_size, self.patch_size, w // self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 4, 1, 3, 5)\n",
    "        return x.flatten(start_dim=3).flatten(start_dim=1, end_dim=2)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.patching_images(inputs)\n",
    "        x = self.init_linear(x)\n",
    "        x = torch.cat([self.xclass.repeat((x.shape[0], 1, 1)), x], dim=1) + self.E_pos\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.mlp_head(x[:, 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(model, train_loader, val_loader, opt, scheduler=None, n_epochs=300, filename='best_transformer.pt'):\n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    with tqdm(range(n_epochs * (len(val_loader) + len(train_loader)))) as pbar:\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            epoch_train_loss = 0\n",
    "            epoch_val_loss = 0\n",
    "            epoch_val_accuracy = 0\n",
    "\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                loss = F.cross_entropy(model(X_batch), y_batch).mean()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "                opt.zero_grad() \n",
    "                epoch_train_loss += loss.item()\n",
    "                pbar.update()\n",
    "            train_loss.append(epoch_train_loss / len(train_loader))\n",
    "            neptune.log_metric('train_loss', train_loss[-1])\n",
    "            print(\"Epoch:\", epoch + 1)\n",
    "            print(\"Train loss: %.3f\" % train_loss[-1])\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for (X_batch, y_batch) in val_loader:\n",
    "                    logits = model(X_batch.to(device))\n",
    "                    y_pred = torch.argmax(logits, dim=1)\n",
    "                    epoch_val_accuracy += np.mean((y_batch == y_pred.cpu()).numpy())\n",
    "                    loss = F.cross_entropy(logits, y_batch.to(device)).mean()\n",
    "                    epoch_val_loss += loss.item()\n",
    "                    pbar.update()\n",
    "                val_accuracy.append(epoch_val_accuracy / len(val_loader))\n",
    "                neptune.log_metric('val_accuracy', val_accuracy[-1])\n",
    "                val_loss.append(epoch_val_loss / len(val_loader))\n",
    "                neptune.log_metric('val_loss', val_loss[-1])\n",
    "                if val_accuracy[-1] > best_accuracy:\n",
    "                    torch.save(model, filename)\n",
    "                    best_accuracy = val_accuracy[-1]\n",
    "                print(\"Val loss: %.3f\" % val_loss[-1])\n",
    "                print(\"Val accuracy: %.3f\" % val_accuracy[-1])\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupSchedule:\n",
    "    def __init__(self, dataset_len, batch_size, n_epochs, warmup_percent=1):\n",
    "        self.n_iterations = n_epochs * (dataset_len // batch_size)\n",
    "        self.warmuplen = (self.n_iterations * warmup_percent) // 100\n",
    "        \n",
    "    def get_lr_coef(self, i):\n",
    "        if i < self.warmuplen:\n",
    "            return (i + 1) / self.warmuplen\n",
    "        return (self.n_iterations - i) / (self.n_iterations - self.warmuplen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There is a new version of neptune-client 0.9.1 (installed: 0.5.5).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Project(calistro/vit)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import neptune\n",
    "NEPTUNE_API_TOKEN = 'sometoken'\n",
    "neptune.init(f'calistro/vit', api_token=NEPTUNE_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=200\n",
    "\n",
    "train_dir = '../../../../data/evdmsivets/tiny-imagenet-200/train'\n",
    "test_dir = '../../../../data/evdmsivets/tiny-imagenet-200/val'\n",
    "\n",
    "train_transforms = torchvision.transforms.Compose([\n",
    "    transforms.RandomRotation([-10, 10]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "val_dataset = torchvision.datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "\n",
    "train_batch_gen = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "val_batch_gen = DataLoader(val_dataset, batch_size=batch_size, num_workers=10)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/calistro/vit/e/VIT-12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf157a68a6f45e0a26f3bee6721dc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=82500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss: 4.862\n",
      "Val loss: 4.545\n",
      "Val accuracy: 0.073\n",
      "Epoch: 2\n",
      "Train loss: 4.278\n",
      "Val loss: 4.007\n",
      "Val accuracy: 0.139\n",
      "Epoch: 3\n",
      "Train loss: 3.909\n",
      "Val loss: 3.732\n",
      "Val accuracy: 0.177\n",
      "Epoch: 4\n",
      "Train loss: 3.556\n",
      "Val loss: 3.457\n",
      "Val accuracy: 0.223\n",
      "Epoch: 5\n",
      "Train loss: 3.253\n",
      "Val loss: 3.218\n",
      "Val accuracy: 0.268\n",
      "Epoch: 6\n",
      "Train loss: 3.032\n",
      "Val loss: 3.018\n",
      "Val accuracy: 0.300\n",
      "Epoch: 7\n",
      "Train loss: 2.848\n",
      "Val loss: 2.899\n",
      "Val accuracy: 0.330\n",
      "Epoch: 8\n",
      "Train loss: 2.690\n",
      "Val loss: 2.867\n",
      "Val accuracy: 0.335\n",
      "Epoch: 9\n",
      "Train loss: 2.537\n",
      "Val loss: 2.805\n",
      "Val accuracy: 0.347\n",
      "Epoch: 10\n",
      "Train loss: 2.379\n",
      "Val loss: 2.715\n",
      "Val accuracy: 0.371\n",
      "Epoch: 11\n",
      "Train loss: 2.223\n",
      "Val loss: 2.718\n",
      "Val accuracy: 0.369\n",
      "Epoch: 12\n",
      "Train loss: 2.054\n",
      "Val loss: 2.721\n",
      "Val accuracy: 0.379\n",
      "Epoch: 13\n",
      "Train loss: 1.884\n",
      "Val loss: 2.778\n",
      "Val accuracy: 0.375\n",
      "Epoch: 14\n",
      "Train loss: 1.700\n",
      "Val loss: 2.840\n",
      "Val accuracy: 0.381\n",
      "Epoch: 15\n",
      "Train loss: 1.512\n",
      "Val loss: 2.965\n",
      "Val accuracy: 0.377\n",
      "Epoch: 16\n",
      "Train loss: 1.344\n",
      "Val loss: 3.144\n",
      "Val accuracy: 0.368\n",
      "Epoch: 17\n",
      "Train loss: 1.170\n",
      "Val loss: 3.299\n",
      "Val accuracy: 0.355\n",
      "Epoch: 18\n",
      "Train loss: 1.004\n",
      "Val loss: 3.473\n",
      "Val accuracy: 0.362\n",
      "Epoch: 19\n",
      "Train loss: 0.859\n",
      "Val loss: 3.704\n",
      "Val accuracy: 0.356\n",
      "Epoch: 20\n",
      "Train loss: 0.748\n",
      "Val loss: 3.846\n",
      "Val accuracy: 0.356\n",
      "Epoch: 21\n",
      "Train loss: 0.653\n",
      "Val loss: 3.922\n",
      "Val accuracy: 0.359\n",
      "Epoch: 22\n",
      "Train loss: 0.567\n",
      "Val loss: 4.118\n",
      "Val accuracy: 0.352\n",
      "Epoch: 23\n",
      "Train loss: 0.508\n",
      "Val loss: 4.301\n",
      "Val accuracy: 0.351\n",
      "Epoch: 24\n",
      "Train loss: 0.451\n",
      "Val loss: 4.422\n",
      "Val accuracy: 0.355\n",
      "Epoch: 25\n",
      "Train loss: 0.422\n",
      "Val loss: 4.477\n",
      "Val accuracy: 0.351\n",
      "Epoch: 26\n",
      "Train loss: 0.376\n",
      "Val loss: 4.661\n",
      "Val accuracy: 0.352\n",
      "Epoch: 27\n",
      "Train loss: 0.350\n",
      "Val loss: 4.911\n",
      "Val accuracy: 0.335\n",
      "Epoch: 28\n",
      "Train loss: 0.346\n",
      "Val loss: 4.864\n",
      "Val accuracy: 0.351\n",
      "Epoch: 29\n",
      "Train loss: 0.295\n",
      "Val loss: 4.956\n",
      "Val accuracy: 0.346\n",
      "Epoch: 30\n",
      "Train loss: 0.279\n",
      "Val loss: 5.042\n",
      "Val accuracy: 0.346\n",
      "Epoch: 31\n",
      "Train loss: 0.281\n",
      "Val loss: 4.987\n",
      "Val accuracy: 0.351\n",
      "Epoch: 32\n",
      "Train loss: 0.266\n",
      "Val loss: 5.223\n",
      "Val accuracy: 0.339\n",
      "Epoch: 33\n",
      "Train loss: 0.247\n",
      "Val loss: 5.064\n",
      "Val accuracy: 0.353\n",
      "Epoch: 34\n",
      "Train loss: 0.234\n",
      "Val loss: 5.361\n",
      "Val accuracy: 0.346\n",
      "Epoch: 35\n",
      "Train loss: 0.223\n",
      "Val loss: 5.363\n",
      "Val accuracy: 0.345\n",
      "Epoch: 36\n",
      "Train loss: 0.254\n",
      "Val loss: 5.259\n",
      "Val accuracy: 0.352\n",
      "Epoch: 37\n",
      "Train loss: 0.203\n",
      "Val loss: 5.357\n",
      "Val accuracy: 0.352\n",
      "Epoch: 38\n",
      "Train loss: 0.184\n",
      "Val loss: 5.437\n",
      "Val accuracy: 0.351\n",
      "Epoch: 39\n",
      "Train loss: 0.179\n",
      "Val loss: 5.441\n",
      "Val accuracy: 0.355\n",
      "Epoch: 40\n",
      "Train loss: 0.186\n",
      "Val loss: 5.516\n",
      "Val accuracy: 0.350\n",
      "Epoch: 41\n",
      "Train loss: 0.177\n",
      "Val loss: 5.546\n",
      "Val accuracy: 0.353\n",
      "Epoch: 42\n",
      "Train loss: 0.171\n",
      "Val loss: 5.623\n",
      "Val accuracy: 0.351\n",
      "Epoch: 43\n",
      "Train loss: 0.163\n",
      "Val loss: 5.570\n",
      "Val accuracy: 0.353\n",
      "Epoch: 44\n",
      "Train loss: 0.159\n",
      "Val loss: 5.546\n",
      "Val accuracy: 0.354\n",
      "Epoch: 45\n",
      "Train loss: 0.151\n",
      "Val loss: 5.815\n",
      "Val accuracy: 0.344\n",
      "Epoch: 46\n",
      "Train loss: 0.143\n",
      "Val loss: 5.761\n",
      "Val accuracy: 0.352\n",
      "Epoch: 47\n",
      "Train loss: 0.143\n",
      "Val loss: 5.806\n",
      "Val accuracy: 0.353\n",
      "Epoch: 48\n",
      "Train loss: 0.133\n",
      "Val loss: 5.811\n",
      "Val accuracy: 0.354\n",
      "Epoch: 49\n",
      "Train loss: 0.132\n",
      "Val loss: 5.816\n",
      "Val accuracy: 0.351\n",
      "Epoch: 50\n",
      "Train loss: 0.131\n",
      "Val loss: 6.104\n",
      "Val accuracy: 0.341\n",
      "Epoch: 51\n",
      "Train loss: 0.125\n",
      "Val loss: 5.898\n",
      "Val accuracy: 0.362\n",
      "Epoch: 52\n",
      "Train loss: 0.118\n",
      "Val loss: 5.936\n",
      "Val accuracy: 0.360\n",
      "Epoch: 53\n",
      "Train loss: 0.117\n",
      "Val loss: 6.009\n",
      "Val accuracy: 0.355\n",
      "Epoch: 54\n",
      "Train loss: 0.114\n",
      "Val loss: 6.020\n",
      "Val accuracy: 0.352\n",
      "Epoch: 55\n",
      "Train loss: 0.110\n",
      "Val loss: 5.860\n",
      "Val accuracy: 0.362\n",
      "Epoch: 56\n",
      "Train loss: 0.100\n",
      "Val loss: 5.985\n",
      "Val accuracy: 0.362\n",
      "Epoch: 57\n",
      "Train loss: 0.098\n",
      "Val loss: 5.953\n",
      "Val accuracy: 0.364\n",
      "Epoch: 58\n",
      "Train loss: 0.097\n",
      "Val loss: 6.072\n",
      "Val accuracy: 0.355\n",
      "Epoch: 59\n",
      "Train loss: 0.100\n",
      "Val loss: 6.232\n",
      "Val accuracy: 0.351\n",
      "Epoch: 60\n",
      "Train loss: 0.094\n",
      "Val loss: 6.166\n",
      "Val accuracy: 0.353\n",
      "Epoch: 61\n",
      "Train loss: 0.086\n",
      "Val loss: 6.140\n",
      "Val accuracy: 0.362\n",
      "Epoch: 62\n",
      "Train loss: 0.084\n",
      "Val loss: 6.102\n",
      "Val accuracy: 0.366\n",
      "Epoch: 63\n",
      "Train loss: 0.084\n",
      "Val loss: 6.067\n",
      "Val accuracy: 0.365\n",
      "Epoch: 64\n",
      "Train loss: 0.082\n",
      "Val loss: 5.961\n",
      "Val accuracy: 0.357\n",
      "Epoch: 65\n",
      "Train loss: 0.084\n",
      "Val loss: 6.226\n",
      "Val accuracy: 0.356\n",
      "Epoch: 66\n",
      "Train loss: 0.080\n",
      "Val loss: 6.568\n",
      "Val accuracy: 0.342\n",
      "Epoch: 67\n",
      "Train loss: 0.072\n",
      "Val loss: 6.185\n",
      "Val accuracy: 0.364\n",
      "Epoch: 68\n",
      "Train loss: 0.071\n",
      "Val loss: 6.387\n",
      "Val accuracy: 0.353\n",
      "Epoch: 69\n",
      "Train loss: 0.072\n",
      "Val loss: 6.279\n",
      "Val accuracy: 0.361\n",
      "Epoch: 70\n",
      "Train loss: 0.067\n",
      "Val loss: 6.250\n",
      "Val accuracy: 0.366\n",
      "Epoch: 71\n",
      "Train loss: 0.065\n",
      "Val loss: 6.236\n",
      "Val accuracy: 0.364\n",
      "Epoch: 72\n",
      "Train loss: 0.065\n",
      "Val loss: 6.294\n",
      "Val accuracy: 0.366\n",
      "Epoch: 73\n",
      "Train loss: 0.060\n",
      "Val loss: 6.323\n",
      "Val accuracy: 0.362\n",
      "Epoch: 74\n",
      "Train loss: 0.056\n",
      "Val loss: 6.465\n",
      "Val accuracy: 0.359\n",
      "Epoch: 75\n",
      "Train loss: 0.064\n",
      "Val loss: 6.394\n",
      "Val accuracy: 0.363\n",
      "Epoch: 76\n",
      "Train loss: 0.060\n",
      "Val loss: 6.173\n",
      "Val accuracy: 0.371\n",
      "Epoch: 77\n",
      "Train loss: 0.052\n",
      "Val loss: 6.517\n",
      "Val accuracy: 0.359\n",
      "Epoch: 78\n",
      "Train loss: 0.054\n",
      "Val loss: 6.598\n",
      "Val accuracy: 0.354\n",
      "Epoch: 79\n",
      "Train loss: 0.050\n",
      "Val loss: 6.527\n",
      "Val accuracy: 0.358\n",
      "Epoch: 80\n",
      "Train loss: 0.052\n",
      "Val loss: 6.528\n",
      "Val accuracy: 0.362\n",
      "Epoch: 81\n",
      "Train loss: 0.047\n",
      "Val loss: 6.520\n",
      "Val accuracy: 0.363\n",
      "Epoch: 82\n",
      "Train loss: 0.047\n",
      "Val loss: 6.652\n",
      "Val accuracy: 0.357\n",
      "Epoch: 83\n",
      "Train loss: 0.042\n",
      "Val loss: 6.474\n",
      "Val accuracy: 0.362\n",
      "Epoch: 84\n",
      "Train loss: 0.042\n",
      "Val loss: 6.366\n",
      "Val accuracy: 0.369\n",
      "Epoch: 85\n",
      "Train loss: 0.040\n",
      "Val loss: 6.548\n",
      "Val accuracy: 0.359\n",
      "Epoch: 86\n",
      "Train loss: 0.040\n",
      "Val loss: 6.629\n",
      "Val accuracy: 0.362\n",
      "Epoch: 87\n",
      "Train loss: 0.042\n",
      "Val loss: 6.626\n",
      "Val accuracy: 0.357\n",
      "Epoch: 88\n",
      "Train loss: 0.040\n",
      "Val loss: 6.561\n",
      "Val accuracy: 0.364\n",
      "Epoch: 89\n",
      "Train loss: 0.036\n",
      "Val loss: 6.720\n",
      "Val accuracy: 0.360\n",
      "Epoch: 90\n",
      "Train loss: 0.034\n",
      "Val loss: 6.617\n",
      "Val accuracy: 0.366\n",
      "Epoch: 91\n",
      "Train loss: 0.036\n",
      "Val loss: 6.679\n",
      "Val accuracy: 0.369\n",
      "Epoch: 92\n",
      "Train loss: 0.033\n",
      "Val loss: 6.578\n",
      "Val accuracy: 0.362\n",
      "Epoch: 93\n",
      "Train loss: 0.032\n",
      "Val loss: 6.569\n",
      "Val accuracy: 0.369\n",
      "Epoch: 94\n",
      "Train loss: 0.031\n",
      "Val loss: 6.712\n",
      "Val accuracy: 0.364\n",
      "Epoch: 95\n",
      "Train loss: 0.032\n",
      "Val loss: 6.648\n",
      "Val accuracy: 0.364\n",
      "Epoch: 96\n",
      "Train loss: 0.029\n",
      "Val loss: 6.768\n",
      "Val accuracy: 0.359\n",
      "Epoch: 97\n",
      "Train loss: 0.029\n",
      "Val loss: 6.723\n",
      "Val accuracy: 0.363\n",
      "Epoch: 98\n",
      "Train loss: 0.028\n",
      "Val loss: 6.740\n",
      "Val accuracy: 0.367\n",
      "Epoch: 99\n",
      "Train loss: 0.027\n",
      "Val loss: 6.744\n",
      "Val accuracy: 0.363\n",
      "Epoch: 100\n",
      "Train loss: 0.025\n",
      "Val loss: 6.699\n",
      "Val accuracy: 0.368\n",
      "Epoch: 101\n",
      "Train loss: 0.023\n",
      "Val loss: 6.733\n",
      "Val accuracy: 0.373\n",
      "Epoch: 102\n",
      "Train loss: 0.023\n",
      "Val loss: 6.808\n",
      "Val accuracy: 0.364\n",
      "Epoch: 103\n",
      "Train loss: 0.021\n",
      "Val loss: 6.697\n",
      "Val accuracy: 0.373\n",
      "Epoch: 104\n",
      "Train loss: 0.020\n",
      "Val loss: 6.720\n",
      "Val accuracy: 0.368\n",
      "Epoch: 105\n",
      "Train loss: 0.021\n",
      "Val loss: 6.771\n",
      "Val accuracy: 0.363\n",
      "Epoch: 106\n",
      "Train loss: 0.021\n",
      "Val loss: 6.947\n",
      "Val accuracy: 0.368\n",
      "Epoch: 107\n",
      "Train loss: 0.019\n",
      "Val loss: 6.749\n",
      "Val accuracy: 0.371\n",
      "Epoch: 108\n",
      "Train loss: 0.018\n",
      "Val loss: 6.863\n",
      "Val accuracy: 0.369\n",
      "Epoch: 109\n",
      "Train loss: 0.016\n",
      "Val loss: 6.874\n",
      "Val accuracy: 0.369\n",
      "Epoch: 110\n",
      "Train loss: 0.017\n",
      "Val loss: 6.757\n",
      "Val accuracy: 0.374\n",
      "Epoch: 111\n",
      "Train loss: 0.016\n",
      "Val loss: 6.887\n",
      "Val accuracy: 0.370\n",
      "Epoch: 112\n",
      "Train loss: 0.015\n",
      "Val loss: 6.792\n",
      "Val accuracy: 0.374\n",
      "Epoch: 113\n",
      "Train loss: 0.015\n",
      "Val loss: 6.701\n",
      "Val accuracy: 0.377\n",
      "Epoch: 114\n",
      "Train loss: 0.013\n",
      "Val loss: 6.893\n",
      "Val accuracy: 0.369\n",
      "Epoch: 115\n",
      "Train loss: 0.014\n",
      "Val loss: 6.872\n",
      "Val accuracy: 0.370\n",
      "Epoch: 116\n",
      "Train loss: 0.013\n",
      "Val loss: 6.924\n",
      "Val accuracy: 0.364\n",
      "Epoch: 117\n",
      "Train loss: 0.014\n",
      "Val loss: 7.035\n",
      "Val accuracy: 0.367\n",
      "Epoch: 118\n",
      "Train loss: 0.011\n",
      "Val loss: 6.950\n",
      "Val accuracy: 0.370\n",
      "Epoch: 119\n",
      "Train loss: 0.011\n",
      "Val loss: 6.850\n",
      "Val accuracy: 0.374\n",
      "Epoch: 120\n",
      "Train loss: 0.011\n",
      "Val loss: 6.934\n",
      "Val accuracy: 0.377\n",
      "Epoch: 121\n",
      "Train loss: 0.009\n",
      "Val loss: 6.798\n",
      "Val accuracy: 0.377\n",
      "Epoch: 122\n",
      "Train loss: 0.009\n",
      "Val loss: 6.796\n",
      "Val accuracy: 0.374\n",
      "Epoch: 123\n",
      "Train loss: 0.008\n",
      "Val loss: 6.882\n",
      "Val accuracy: 0.378\n",
      "Epoch: 124\n",
      "Train loss: 0.007\n",
      "Val loss: 6.991\n",
      "Val accuracy: 0.373\n",
      "Epoch: 125\n",
      "Train loss: 0.009\n",
      "Val loss: 6.860\n",
      "Val accuracy: 0.376\n",
      "Epoch: 126\n",
      "Train loss: 0.007\n",
      "Val loss: 6.989\n",
      "Val accuracy: 0.378\n",
      "Epoch: 127\n",
      "Train loss: 0.006\n",
      "Val loss: 6.941\n",
      "Val accuracy: 0.373\n",
      "Epoch: 128\n",
      "Train loss: 0.008\n",
      "Val loss: 6.930\n",
      "Val accuracy: 0.376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129\n",
      "Train loss: 0.006\n",
      "Val loss: 6.957\n",
      "Val accuracy: 0.375\n",
      "Epoch: 130\n",
      "Train loss: 0.006\n",
      "Val loss: 6.920\n",
      "Val accuracy: 0.379\n",
      "Epoch: 131\n",
      "Train loss: 0.005\n",
      "Val loss: 7.046\n",
      "Val accuracy: 0.375\n",
      "Epoch: 132\n",
      "Train loss: 0.005\n",
      "Val loss: 6.946\n",
      "Val accuracy: 0.380\n",
      "Epoch: 133\n",
      "Train loss: 0.005\n",
      "Val loss: 6.991\n",
      "Val accuracy: 0.377\n",
      "Epoch: 134\n",
      "Train loss: 0.005\n",
      "Val loss: 7.009\n",
      "Val accuracy: 0.374\n",
      "Epoch: 135\n",
      "Train loss: 0.004\n",
      "Val loss: 6.938\n",
      "Val accuracy: 0.378\n",
      "Epoch: 136\n",
      "Train loss: 0.004\n",
      "Val loss: 7.060\n",
      "Val accuracy: 0.376\n",
      "Epoch: 137\n",
      "Train loss: 0.004\n",
      "Val loss: 7.009\n",
      "Val accuracy: 0.378\n",
      "Epoch: 138\n",
      "Train loss: 0.004\n",
      "Val loss: 6.892\n",
      "Val accuracy: 0.382\n",
      "Epoch: 139\n",
      "Train loss: 0.003\n",
      "Val loss: 6.932\n",
      "Val accuracy: 0.382\n",
      "Epoch: 140\n",
      "Train loss: 0.003\n",
      "Val loss: 6.890\n",
      "Val accuracy: 0.383\n",
      "Epoch: 141\n",
      "Train loss: 0.003\n",
      "Val loss: 6.910\n",
      "Val accuracy: 0.385\n",
      "Epoch: 142\n",
      "Train loss: 0.003\n",
      "Val loss: 6.935\n",
      "Val accuracy: 0.381\n",
      "Epoch: 143\n",
      "Train loss: 0.003\n",
      "Val loss: 6.837\n",
      "Val accuracy: 0.385\n",
      "Epoch: 144\n",
      "Train loss: 0.002\n",
      "Val loss: 6.908\n",
      "Val accuracy: 0.383\n",
      "Epoch: 145\n",
      "Train loss: 0.002\n",
      "Val loss: 6.892\n",
      "Val accuracy: 0.382\n",
      "Epoch: 146\n",
      "Train loss: 0.002\n",
      "Val loss: 6.939\n",
      "Val accuracy: 0.382\n",
      "Epoch: 147\n",
      "Train loss: 0.002\n",
      "Val loss: 6.922\n",
      "Val accuracy: 0.381\n",
      "Epoch: 148\n",
      "Train loss: 0.002\n",
      "Val loss: 6.890\n",
      "Val accuracy: 0.383\n",
      "Epoch: 149\n",
      "Train loss: 0.002\n",
      "Val loss: 6.887\n",
      "Val accuracy: 0.384\n",
      "Epoch: 150\n",
      "Train loss: 0.001\n",
      "Val loss: 6.890\n",
      "Val accuracy: 0.383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device='cuda:5'\n",
    "\n",
    "model = ViT(n_classes=200, dim=384, mlp_size=1536, n_patches=64, patch_size=8,\n",
    "            depth=6, n_heads=6, n_channels=3, p_drop=0.05).to(device)\n",
    "\n",
    "neptune.create_experiment()\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "  \n",
    "n_epochs = 150\n",
    "lr_schedule = WarmupSchedule(len(dataset), batch_size, n_epochs, warmup_percent=2)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lr_schedule.get_lr_coef)\n",
    "model = train(model, train_batch_gen, val_batch_gen, opt, scheduler, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was trained from scratch on the TinyImageNet dataset, achieving a validation accuracy of 0.38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
